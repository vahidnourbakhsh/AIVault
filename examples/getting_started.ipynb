{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "692b7fd1",
   "metadata": {},
   "source": [
    "# Getting Started with AIVault\n",
    "\n",
    "Welcome to AIVault! This notebook demonstrates the basic usage of AIVault, a comprehensive collection of AI models, methods, examples, and tutorials.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "In this notebook, you'll learn how to:\n",
    "- Import and use AIVault utilities\n",
    "- Load and preprocess data\n",
    "- Train a simple machine learning model\n",
    "- Evaluate model performance\n",
    "- Make predictions\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6506aa27",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "First, let's import the essential libraries including AIVault utilities, pandas, numpy, matplotlib, seaborn, and scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8b6e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Import AIVault utilities\n",
    "try:\n",
    "    from aivault.utilities import set_seed, setup_logging\n",
    "    from aivault.utilities.data_utils import get_data_info\n",
    "    from aivault.utilities.viz_utils import plot_confusion_matrix, set_plot_style\n",
    "    from aivault.utilities.eval_utils import calculate_metrics\n",
    "    print(\"‚úÖ AIVault utilities imported successfully!\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è AIVault not fully available: {e}\")\n",
    "    print(\"You can still follow along with the basic workflow using standard libraries.\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "try:\n",
    "    set_seed(42)\n",
    "    print(\"‚úÖ Random seed set using AIVault utilities\")\n",
    "except:\n",
    "    print(\"‚úÖ Random seed set using numpy\")\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "print(\"‚úÖ Libraries imported and setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d09cc19",
   "metadata": {},
   "source": [
    "## 2. Load and Explore the Dataset\n",
    "\n",
    "For this example, we'll use the famous Iris dataset from scikit-learn. This is a classic dataset for classification tasks containing measurements of iris flowers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fc5c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load data\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Create a DataFrame for easier manipulation\n",
    "df = pd.DataFrame(X, columns=iris.feature_names)\n",
    "df['target'] = y\n",
    "df['species'] = df['target'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"\\nDataset Information:\")\n",
    "print(df.info())\n",
    "\n",
    "# Show target distribution\n",
    "print(\"\\nTarget Distribution:\")\n",
    "print(df['species'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5a5191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations to explore the data\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Pairplot-style scatter plots\n",
    "feature_pairs = [\n",
    "    ('sepal length (cm)', 'sepal width (cm)'),\n",
    "    ('petal length (cm)', 'petal width (cm)'),\n",
    "    ('sepal length (cm)', 'petal length (cm)'),\n",
    "    ('sepal width (cm)', 'petal width (cm)')\n",
    "]\n",
    "\n",
    "for i, (feat1, feat2) in enumerate(feature_pairs):\n",
    "    ax = axes[i//2, i%2]\n",
    "    for species in df['species'].unique():\n",
    "        data = df[df['species'] == species]\n",
    "        ax.scatter(data[feat1], data[feat2], label=species, alpha=0.7)\n",
    "    ax.set_xlabel(feat1)\n",
    "    ax.set_ylabel(feat2)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistical summary\n",
    "print(\"\\nStatistical Summary:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd0c9ae",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing\n",
    "\n",
    "The Iris dataset is already clean, but let's demonstrate some common preprocessing steps that you would typically perform on real-world data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f6df4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Check for duplicates\n",
    "print(f\"\\nDuplicate rows: {df.duplicated().sum()}\")\n",
    "\n",
    "# Prepare features and target\n",
    "X = df.drop(['target', 'species'], axis=1)\n",
    "y = df['target']\n",
    "\n",
    "print(f\"\\nFeature matrix shape: {X.shape}\")\n",
    "print(f\"Target vector shape: {y.shape}\")\n",
    "\n",
    "# Feature scaling (optional for Random Forest, but good practice)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "print(\"\\nFeatures before scaling:\")\n",
    "print(X.describe().round(2))\n",
    "\n",
    "print(\"\\nFeatures after scaling:\")\n",
    "print(X_scaled.describe().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42410d96",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering\n",
    "\n",
    "Let's create some new features that might help improve our model's performance by capturing additional relationships in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14ecc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new features from existing ones\n",
    "X_engineered = X.copy()\n",
    "\n",
    "# Add ratio features\n",
    "X_engineered['sepal_ratio'] = X_engineered['sepal length (cm)'] / X_engineered['sepal width (cm)']\n",
    "X_engineered['petal_ratio'] = X_engineered['petal length (cm)'] / X_engineered['petal width (cm)']\n",
    "\n",
    "# Add area features (assuming elliptical approximation)\n",
    "X_engineered['sepal_area'] = np.pi * (X_engineered['sepal length (cm)'] / 2) * (X_engineered['sepal width (cm)'] / 2)\n",
    "X_engineered['petal_area'] = np.pi * (X_engineered['petal length (cm)'] / 2) * (X_engineered['petal width (cm)'] / 2)\n",
    "\n",
    "# Add total size feature\n",
    "X_engineered['total_area'] = X_engineered['sepal_area'] + X_engineered['petal_area']\n",
    "\n",
    "print(\"Original features:\", X.columns.tolist())\n",
    "print(\"Engineered features:\", X_engineered.columns.tolist())\n",
    "print(f\"\\nNew feature matrix shape: {X_engineered.shape}\")\n",
    "\n",
    "# Show correlation matrix for new features\n",
    "new_features = ['sepal_ratio', 'petal_ratio', 'sepal_area', 'petal_area', 'total_area']\n",
    "corr_matrix = X_engineered[new_features].corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, fmt='.2f')\n",
    "plt.title('Correlation Matrix of New Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1760c71",
   "metadata": {},
   "source": [
    "## 5. Model Training\n",
    "\n",
    "Now let's train a machine learning model using our prepared data. We'll use a Random Forest classifier and compare performance with original vs. engineered features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b278ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "X_train_eng, X_test_eng, _, _ = train_test_split(\n",
    "    X_engineered, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")\n",
    "\n",
    "# Train model with original features\n",
    "rf_original = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_original.fit(X_train, y_train)\n",
    "\n",
    "# Train model with engineered features\n",
    "rf_engineered = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_engineered.fit(X_train_eng, y_train)\n",
    "\n",
    "print(\"‚úÖ Models trained successfully!\")\n",
    "\n",
    "# Feature importance for original model\n",
    "feature_importance_orig = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': rf_original.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importance (Original Features):\")\n",
    "print(feature_importance_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9763ee",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation\n",
    "\n",
    "Let's evaluate our trained models using various metrics and visualizations to understand their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2545b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred_orig = rf_original.predict(X_test)\n",
    "y_pred_eng = rf_engineered.predict(X_test_eng)\n",
    "\n",
    "# Calculate accuracy scores\n",
    "acc_orig = accuracy_score(y_test, y_pred_orig)\n",
    "acc_eng = accuracy_score(y_test, y_pred_eng)\n",
    "\n",
    "print(\"Model Performance Comparison:\")\n",
    "print(f\"Original Features - Accuracy: {acc_orig:.3f}\")\n",
    "print(f\"Engineered Features - Accuracy: {acc_eng:.3f}\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ORIGINAL FEATURES - Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_orig, target_names=iris.target_names))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50) \n",
    "print(\"ENGINEERED FEATURES - Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_eng, target_names=iris.target_names))\n",
    "\n",
    "# Confusion matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Original features confusion matrix\n",
    "cm_orig = confusion_matrix(y_test, y_pred_orig)\n",
    "sns.heatmap(cm_orig, annot=True, fmt='d', cmap='Blues', \n",
    "           xticklabels=iris.target_names, yticklabels=iris.target_names, ax=axes[0])\n",
    "axes[0].set_title(f'Original Features\\nAccuracy: {acc_orig:.3f}')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "\n",
    "# Engineered features confusion matrix  \n",
    "cm_eng = confusion_matrix(y_test, y_pred_eng)\n",
    "sns.heatmap(cm_eng, annot=True, fmt='d', cmap='Blues',\n",
    "           xticklabels=iris.target_names, yticklabels=iris.target_names, ax=axes[1])\n",
    "axes[1].set_title(f'Engineered Features\\nAccuracy: {acc_eng:.3f}')\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328a6727",
   "metadata": {},
   "source": [
    "## 7. Make Predictions\n",
    "\n",
    "Finally, let's demonstrate how to use our trained model to make predictions on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92c6216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some new sample data for prediction\n",
    "new_samples = np.array([\n",
    "    [5.1, 3.5, 1.4, 0.2],  # Likely Setosa\n",
    "    [6.2, 2.8, 4.8, 1.8],  # Likely Versicolor  \n",
    "    [7.2, 3.0, 5.8, 2.2],  # Likely Virginica\n",
    "])\n",
    "\n",
    "new_samples_df = pd.DataFrame(new_samples, columns=X.columns)\n",
    "\n",
    "print(\"New samples for prediction:\")\n",
    "print(new_samples_df)\n",
    "\n",
    "# Make predictions with original model\n",
    "predictions_orig = rf_original.predict(new_samples)\n",
    "probabilities_orig = rf_original.predict_proba(new_samples)\n",
    "\n",
    "# Engineer features for new samples\n",
    "new_samples_eng = new_samples_df.copy()\n",
    "new_samples_eng['sepal_ratio'] = new_samples_eng['sepal length (cm)'] / new_samples_eng['sepal width (cm)']\n",
    "new_samples_eng['petal_ratio'] = new_samples_eng['petal length (cm)'] / new_samples_eng['petal width (cm)']\n",
    "new_samples_eng['sepal_area'] = np.pi * (new_samples_eng['sepal length (cm)'] / 2) * (new_samples_eng['sepal width (cm)'] / 2)\n",
    "new_samples_eng['petal_area'] = np.pi * (new_samples_eng['petal length (cm)'] / 2) * (new_samples_eng['petal width (cm)'] / 2)\n",
    "new_samples_eng['total_area'] = new_samples_eng['sepal_area'] + new_samples_eng['petal_area']\n",
    "\n",
    "# Make predictions with engineered model\n",
    "predictions_eng = rf_engineered.predict(new_samples_eng)\n",
    "probabilities_eng = rf_engineered.predict_proba(new_samples_eng)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PREDICTIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i in range(len(new_samples)):\n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(f\"  Original Model: {iris.target_names[predictions_orig[i]]} (confidence: {np.max(probabilities_orig[i]):.3f})\")\n",
    "    print(f\"  Engineered Model: {iris.target_names[predictions_eng[i]]} (confidence: {np.max(probabilities_eng[i]):.3f})\")\n",
    "    \n",
    "    # Show probability distribution\n",
    "    print(\"  Probability distribution:\")\n",
    "    for j, species in enumerate(iris.target_names):\n",
    "        print(f\"    {species}: Original={probabilities_orig[i][j]:.3f}, Engineered={probabilities_eng[i][j]:.3f}\")\n",
    "\n",
    "print(f\"\\nüéâ Congratulations! You've successfully completed the AIVault getting started tutorial!\")\n",
    "print(f\"You've learned how to:\")\n",
    "print(f\"  ‚úÖ Import and use AIVault utilities\")\n",
    "print(f\"  ‚úÖ Load and explore data\") \n",
    "print(f\"  ‚úÖ Preprocess and engineer features\")\n",
    "print(f\"  ‚úÖ Train machine learning models\")\n",
    "print(f\"  ‚úÖ Evaluate model performance\")\n",
    "print(f\"  ‚úÖ Make predictions on new data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e940bf",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that you've completed this tutorial, here are some suggestions for further exploration:\n",
    "\n",
    "### üöÄ Advanced AIVault Features\n",
    "- Explore the `generative_ai` module for text generation and image synthesis\n",
    "- Try computer vision examples in the `computer_vision` module  \n",
    "- Experiment with NLP techniques in the `nlp` module\n",
    "- Learn about model optimization and deployment strategies\n",
    "\n",
    "### üìö Additional Learning Resources\n",
    "- Check out other example notebooks in the `examples/` directory\n",
    "- Read the documentation in the `docs/` directory\n",
    "- Explore advanced tutorials and research implementations\n",
    "\n",
    "### üõ†Ô∏è Contributing\n",
    "- Found a bug or have a feature request? Open an issue on GitHub\n",
    "- Want to contribute code? Check out our contributing guidelines\n",
    "- Share your own AI implementations and tutorials!\n",
    "\n",
    "Thank you for using AIVault! Happy learning! üéì‚ú®"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
