{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "692b7fd1",
   "metadata": {},
   "source": [
    "# Getting Started with AIVault\n",
    "\n",
    "Welcome to AIVault! This notebook demonstrates the basic usage of AIVault, a comprehensive collection of AI models, methods, examples, and tutorials.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "In this notebook, you'll learn how to:\n",
    "- Import and use AIVault utilities\n",
    "- Load and preprocess data\n",
    "- Train a simple machine learning model\n",
    "- Evaluate model performance\n",
    "- Make predictions\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6506aa27",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "First, let's import the essential libraries including AIVault utilities, pandas, numpy, matplotlib, seaborn, and scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8b6e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Import AIVault utilities\n",
    "try:\n",
    "    from aivault.utilities import set_seed, setup_logging\n",
    "    from aivault.utilities.data_utils import get_data_info\n",
    "    from aivault.utilities.viz_utils import plot_confusion_matrix, set_plot_style\n",
    "    from aivault.utilities.eval_utils import calculate_metrics\n",
    "    print(\"✅ AIVault utilities imported successfully!\")\n",
    "except ImportError as e:\n",
    "    print(f\"⚠️ AIVault not fully available: {e}\")\n",
    "    print(\"You can still follow along with the basic workflow using standard libraries.\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "try:\n",
    "    set_seed(42)\n",
    "    print(\"✅ Random seed set using AIVault utilities\")\n",
    "except:\n",
    "    print(\"✅ Random seed set using numpy\")\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "print(\"✅ Libraries imported and setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d09cc19",
   "metadata": {},
   "source": [
    "## 2. Load and Explore the Dataset\n",
    "\n",
    "For this example, we'll use the famous Iris dataset from scikit-learn. This is a classic dataset for classification tasks containing measurements of iris flowers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fc5c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load data\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Create a DataFrame for easier manipulation\n",
    "df = pd.DataFrame(X, columns=iris.feature_names)\n",
    "df['target'] = y\n",
    "df['species'] = df['target'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"\\nDataset Information:\")\n",
    "print(df.info())\n",
    "\n",
    "# Show target distribution\n",
    "print(\"\\nTarget Distribution:\")\n",
    "print(df['species'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5a5191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations to explore the data\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Pairplot-style scatter plots\n",
    "feature_pairs = [\n",
    "    ('sepal length (cm)', 'sepal width (cm)'),\n",
    "    ('petal length (cm)', 'petal width (cm)'),\n",
    "    ('sepal length (cm)', 'petal length (cm)'),\n",
    "    ('sepal width (cm)', 'petal width (cm)')\n",
    "]\n",
    "\n",
    "for i, (feat1, feat2) in enumerate(feature_pairs):\n",
    "    ax = axes[i//2, i%2]\n",
    "    for species in df['species'].unique():\n",
    "        data = df[df['species'] == species]\n",
    "        ax.scatter(data[feat1], data[feat2], label=species, alpha=0.7)\n",
    "    ax.set_xlabel(feat1)\n",
    "    ax.set_ylabel(feat2)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistical summary\n",
    "print(\"\\nStatistical Summary:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd0c9ae",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing\n",
    "\n",
    "The Iris dataset is already clean, but let's demonstrate some common preprocessing steps that you would typically perform on real-world data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f6df4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Check for duplicates\n",
    "print(f\"\\nDuplicate rows: {df.duplicated().sum()}\")\n",
    "\n",
    "# Prepare features and target\n",
    "X = df.drop(['target', 'species'], axis=1)\n",
    "y = df['target']\n",
    "\n",
    "print(f\"\\nFeature matrix shape: {X.shape}\")\n",
    "print(f\"Target vector shape: {y.shape}\")\n",
    "\n",
    "# Feature scaling (optional for Random Forest, but good practice)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "print(\"\\nFeatures before scaling:\")\n",
    "print(X.describe().round(2))\n",
    "\n",
    "print(\"\\nFeatures after scaling:\")\n",
    "print(X_scaled.describe().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42410d96",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering\n",
    "\n",
    "Let's create some new features that might help improve our model's performance by capturing additional relationships in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14ecc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new features from existing ones\n",
    "X_engineered = X.copy()\n",
    "\n",
    "# Add ratio features\n",
    "X_engineered['sepal_ratio'] = X_engineered['sepal length (cm)'] / X_engineered['sepal width (cm)']\n",
    "X_engineered['petal_ratio'] = X_engineered['petal length (cm)'] / X_engineered['petal width (cm)']\n",
    "\n",
    "# Add area features (assuming elliptical approximation)\n",
    "X_engineered['sepal_area'] = np.pi * (X_engineered['sepal length (cm)'] / 2) * (X_engineered['sepal width (cm)'] / 2)\n",
    "X_engineered['petal_area'] = np.pi * (X_engineered['petal length (cm)'] / 2) * (X_engineered['petal width (cm)'] / 2)\n",
    "\n",
    "# Add total size feature\n",
    "X_engineered['total_area'] = X_engineered['sepal_area'] + X_engineered['petal_area']\n",
    "\n",
    "print(\"Original features:\", X.columns.tolist())\n",
    "print(\"Engineered features:\", X_engineered.columns.tolist())\n",
    "print(f\"\\nNew feature matrix shape: {X_engineered.shape}\")\n",
    "\n",
    "# Show correlation matrix for new features\n",
    "new_features = ['sepal_ratio', 'petal_ratio', 'sepal_area', 'petal_area', 'total_area']\n",
    "corr_matrix = X_engineered[new_features].corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, fmt='.2f')\n",
    "plt.title('Correlation Matrix of New Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1760c71",
   "metadata": {},
   "source": [
    "## 5. Model Training\n",
    "\n",
    "Now let's train a machine learning model using our prepared data. We'll use a Random Forest classifier and compare performance with original vs. engineered features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b278ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "X_train_eng, X_test_eng, _, _ = train_test_split(\n",
    "    X_engineered, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")\n",
    "\n",
    "# Train model with original features\n",
    "rf_original = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_original.fit(X_train, y_train)\n",
    "\n",
    "# Train model with engineered features\n",
    "rf_engineered = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_engineered.fit(X_train_eng, y_train)\n",
    "\n",
    "print(\"✅ Models trained successfully!\")\n",
    "\n",
    "# Feature importance for original model\n",
    "feature_importance_orig = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': rf_original.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importance (Original Features):\")\n",
    "print(feature_importance_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9763ee",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation\n",
    "\n",
    "Let's evaluate our trained models using various metrics and visualizations to understand their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2545b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred_orig = rf_original.predict(X_test)\n",
    "y_pred_eng = rf_engineered.predict(X_test_eng)\n",
    "\n",
    "# Calculate accuracy scores\n",
    "acc_orig = accuracy_score(y_test, y_pred_orig)\n",
    "acc_eng = accuracy_score(y_test, y_pred_eng)\n",
    "\n",
    "print(\"Model Performance Comparison:\")\n",
    "print(f\"Original Features - Accuracy: {acc_orig:.3f}\")\n",
    "print(f\"Engineered Features - Accuracy: {acc_eng:.3f}\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ORIGINAL FEATURES - Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_orig, target_names=iris.target_names))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50) \n",
    "print(\"ENGINEERED FEATURES - Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_eng, target_names=iris.target_names))\n",
    "\n",
    "# Confusion matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Original features confusion matrix\n",
    "cm_orig = confusion_matrix(y_test, y_pred_orig)\n",
    "sns.heatmap(cm_orig, annot=True, fmt='d', cmap='Blues', \n",
    "           xticklabels=iris.target_names, yticklabels=iris.target_names, ax=axes[0])\n",
    "axes[0].set_title(f'Original Features\\nAccuracy: {acc_orig:.3f}')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "\n",
    "# Engineered features confusion matrix  \n",
    "cm_eng = confusion_matrix(y_test, y_pred_eng)\n",
    "sns.heatmap(cm_eng, annot=True, fmt='d', cmap='Blues',\n",
    "           xticklabels=iris.target_names, yticklabels=iris.target_names, ax=axes[1])\n",
    "axes[1].set_title(f'Engineered Features\\nAccuracy: {acc_eng:.3f}')\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328a6727",
   "metadata": {},
   "source": [
    "## 7. Make Predictions\n",
    "\n",
    "Finally, let's demonstrate how to use our trained model to make predictions on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92c6216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some new sample data for prediction\n",
    "new_samples = np.array([\n",
    "    [5.1, 3.5, 1.4, 0.2],  # Likely Setosa\n",
    "    [6.2, 2.8, 4.8, 1.8],  # Likely Versicolor  \n",
    "    [7.2, 3.0, 5.8, 2.2],  # Likely Virginica\n",
    "])\n",
    "\n",
    "new_samples_df = pd.DataFrame(new_samples, columns=X.columns)\n",
    "\n",
    "print(\"New samples for prediction:\")\n",
    "print(new_samples_df)\n",
    "\n",
    "# Make predictions with original model\n",
    "predictions_orig = rf_original.predict(new_samples)\n",
    "probabilities_orig = rf_original.predict_proba(new_samples)\n",
    "\n",
    "# Engineer features for new samples\n",
    "new_samples_eng = new_samples_df.copy()\n",
    "new_samples_eng['sepal_ratio'] = new_samples_eng['sepal length (cm)'] / new_samples_eng['sepal width (cm)']\n",
    "new_samples_eng['petal_ratio'] = new_samples_eng['petal length (cm)'] / new_samples_eng['petal width (cm)']\n",
    "new_samples_eng['sepal_area'] = np.pi * (new_samples_eng['sepal length (cm)'] / 2) * (new_samples_eng['sepal width (cm)'] / 2)\n",
    "new_samples_eng['petal_area'] = np.pi * (new_samples_eng['petal length (cm)'] / 2) * (new_samples_eng['petal width (cm)'] / 2)\n",
    "new_samples_eng['total_area'] = new_samples_eng['sepal_area'] + new_samples_eng['petal_area']\n",
    "\n",
    "# Make predictions with engineered model\n",
    "predictions_eng = rf_engineered.predict(new_samples_eng)\n",
    "probabilities_eng = rf_engineered.predict_proba(new_samples_eng)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PREDICTIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i in range(len(new_samples)):\n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(f\"  Original Model: {iris.target_names[predictions_orig[i]]} (confidence: {np.max(probabilities_orig[i]):.3f})\")\n",
    "    print(f\"  Engineered Model: {iris.target_names[predictions_eng[i]]} (confidence: {np.max(probabilities_eng[i]):.3f})\")\n",
    "    \n",
    "    # Show probability distribution\n",
    "    print(\"  Probability distribution:\")\n",
    "    for j, species in enumerate(iris.target_names):\n",
    "        print(f\"    {species}: Original={probabilities_orig[i][j]:.3f}, Engineered={probabilities_eng[i][j]:.3f}\")\n",
    "\n",
    "print(f\"\\n🎉 Congratulations! You've successfully completed the AIVault getting started tutorial!\")\n",
    "print(f\"You've learned how to:\")\n",
    "print(f\"  ✅ Import and use AIVault utilities\")\n",
    "print(f\"  ✅ Load and explore data\") \n",
    "print(f\"  ✅ Preprocess and engineer features\")\n",
    "print(f\"  ✅ Train machine learning models\")\n",
    "print(f\"  ✅ Evaluate model performance\")\n",
    "print(f\"  ✅ Make predictions on new data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e940bf",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that you've completed this tutorial, here are some suggestions for further exploration:\n",
    "\n",
    "### 🚀 Advanced AIVault Features\n",
    "- Explore the `generative_ai` module for text generation and image synthesis\n",
    "- Try computer vision examples in the `computer_vision` module  \n",
    "- Experiment with NLP techniques in the `nlp` module\n",
    "- Learn about model optimization and deployment strategies\n",
    "\n",
    "### 📚 Additional Learning Resources\n",
    "- Check out other example notebooks in the `examples/` directory\n",
    "- Read the documentation in the `docs/` directory\n",
    "- Explore advanced tutorials and research implementations\n",
    "\n",
    "### 🛠️ Contributing\n",
    "- Found a bug or have a feature request? Open an issue on GitHub\n",
    "- Want to contribute code? Check out our contributing guidelines\n",
    "- Share your own AI implementations and tutorials!\n",
    "\n",
    "Thank you for using AIVault! Happy learning! 🎓✨"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
